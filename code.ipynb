{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for the adult data:\n",
      "                age        fnlwgt  education_num  capital_gain  capital_loss  \\\n",
      "count  32561.000000  3.256100e+04   32561.000000  32561.000000  32561.000000   \n",
      "mean      38.581647  1.897784e+05      10.080679   1077.648844     87.303830   \n",
      "std       13.640433  1.055500e+05       2.572720   7385.292085    402.960219   \n",
      "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
      "25%       28.000000  1.178270e+05       9.000000      0.000000      0.000000   \n",
      "50%       37.000000  1.783560e+05      10.000000      0.000000      0.000000   \n",
      "75%       48.000000  2.370510e+05      12.000000      0.000000      0.000000   \n",
      "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
      "\n",
      "       hours_per_week  \n",
      "count    32561.000000  \n",
      "mean        40.437456  \n",
      "std         12.347429  \n",
      "min          1.000000  \n",
      "25%         40.000000  \n",
      "50%         40.000000  \n",
      "75%         45.000000  \n",
      "max         99.000000  \n",
      "\n",
      "Summary statistics for the adult test\n",
      "                age        fnlwgt  education_num  capital_gain  capital_loss  \\\n",
      "count  16281.000000  1.628100e+04   16281.000000  16281.000000  16281.000000   \n",
      "mean      38.767459  1.894357e+05      10.072907   1081.905104     87.899269   \n",
      "std       13.849187  1.057149e+05       2.567545   7583.935968    403.105286   \n",
      "min       17.000000  1.349200e+04       1.000000      0.000000      0.000000   \n",
      "25%       28.000000  1.167360e+05       9.000000      0.000000      0.000000   \n",
      "50%       37.000000  1.778310e+05      10.000000      0.000000      0.000000   \n",
      "75%       48.000000  2.383840e+05      12.000000      0.000000      0.000000   \n",
      "max       90.000000  1.490400e+06      16.000000  99999.000000   3770.000000   \n",
      "\n",
      "       hours_per_week  \n",
      "count    16281.000000  \n",
      "mean        40.392236  \n",
      "std         12.479332  \n",
      "min          1.000000  \n",
      "25%         40.000000  \n",
      "50%         40.000000  \n",
      "75%         45.000000  \n",
      "max         99.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Part 1 Implementation\n",
    "# File paths\n",
    "data_path = '/Users/anwayatkekar/Downloads/assignment2 (1)/adult.data'\n",
    "test_path = '/Users/anwayatkekar/Downloads/assignment2 (1)/adult.test'\n",
    "\n",
    "\n",
    "# Column names based on the adult.names files\n",
    "column = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "    \"marital_status\", \"occupation\", \"relationship\", \"race\",\n",
    "    \"sex\", \"capital_gain\", \"capital_loss\", \"hours_per_week\",\n",
    "    \"native_country\", \"income\"\n",
    "]\n",
    "\n",
    "# Loading the datasets\n",
    "adultdata = pd.read_csv(data_path, names=column, na_values=\"?\", skipinitialspace=True)\n",
    "adulttest = pd.read_csv(test_path, names=column, na_values=\"?\", skipinitialspace=True, skiprows=1)\n",
    "\n",
    "print(\"Summary statistics for the adult data:\")\n",
    "print(adultdata.describe())\n",
    "\n",
    "print(\"\\nSummary statistics for the adult test\")\n",
    "print(adulttest.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names Summary:\n",
      "Adult.data:\n",
      "age: 0\n",
      "workclass: 1836\n",
      "fnlwgt: 0\n",
      "education: 0\n",
      "education_num: 0\n",
      "marital_status: 0\n",
      "occupation: 1843\n",
      "relationship: 0\n",
      "race: 0\n",
      "sex: 0\n",
      "capital_gain: 0\n",
      "capital_loss: 0\n",
      "hours_per_week: 0\n",
      "native_country: 583\n",
      "income: 0\n",
      "\n",
      "Adult.test:\n",
      "age: 0\n",
      "workclass: 963\n",
      "fnlwgt: 0\n",
      "education: 0\n",
      "education_num: 0\n",
      "marital_status: 0\n",
      "occupation: 966\n",
      "relationship: 0\n",
      "race: 0\n",
      "sex: 0\n",
      "capital_gain: 0\n",
      "capital_loss: 0\n",
      "hours_per_week: 0\n",
      "native_country: 274\n",
      "income: 0\n"
     ]
    }
   ],
   "source": [
    "# Standardizing the income column in the test dataset\n",
    "adulttest['income'] = adulttest['income'].str.replace('.', '', regex=False)\n",
    "\n",
    "# Checking for missing values in both datasets\n",
    "missingdata = adultdata.isnull().sum()\n",
    "missingtest = adulttest.isnull().sum()\n",
    "\n",
    "# Outputting the summary of missing values for both datasets\n",
    "print(\"Column names Summary:\")\n",
    "print(\"Adult.data:\")\n",
    "for column, value in missingdata.items():\n",
    "    print(f\"{column}: {value}\")\n",
    "\n",
    "print(\"\\nAdult.test:\")\n",
    "for column, value in missingtest.items():\n",
    "    print(f\"{column}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Summary After Imputation:\n",
      "adult.data:\n",
      "age: 0\n",
      "workclass: 0\n",
      "fnlwgt: 0\n",
      "education: 0\n",
      "education_num: 0\n",
      "marital_status: 0\n",
      "occupation: 0\n",
      "relationship: 0\n",
      "race: 0\n",
      "sex: 0\n",
      "capital_gain: 0\n",
      "capital_loss: 0\n",
      "hours_per_week: 0\n",
      "native_country: 0\n",
      "income: 0\n",
      "\n",
      "adult.test:\n",
      "age: 0\n",
      "workclass: 0\n",
      "fnlwgt: 0\n",
      "education: 0\n",
      "education_num: 0\n",
      "marital_status: 0\n",
      "occupation: 0\n",
      "relationship: 0\n",
      "race: 0\n",
      "sex: 0\n",
      "capital_gain: 0\n",
      "capital_loss: 0\n",
      "hours_per_week: 0\n",
      "native_country: 0\n",
      "income: 0\n"
     ]
    }
   ],
   "source": [
    "# Imputing missing values with the most frequent category for each column with missing values\n",
    "for column in ['workclass', 'occupation', 'native_country']:\n",
    "    data_m = adultdata[column].mode()[0]\n",
    "    test_m = adulttest[column].mode()[0]\n",
    "    \n",
    "    adultdata.loc[adultdata[column].isnull(), column] = data_m\n",
    "    adulttest.loc[adulttest[column].isnull(), column] = test_m\n",
    "\n",
    "# Rechecking missing values after imputation\n",
    "missingdata_after = adultdata.isnull().sum()\n",
    "missingtest_after = adulttest.isnull().sum()\n",
    "\n",
    "# Outputting the summary of missing values after imputation\n",
    "print(\"Missing Values Summary After Imputation:\")\n",
    "print(\"adult.data:\")\n",
    "for column, value in missingdata_after.items():\n",
    "    print(f\"{column}: {value}\")\n",
    "\n",
    "print(\"\\nadult.test:\")\n",
    "for column, value in missingtest_after.items():\n",
    "    print(f\"{column}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "#Category columns\n",
    "cols_c = [col for col in adultdata.columns if adultdata[col].dtype == \"object\" and col != \"income\"]\n",
    "\n",
    "e1= OneHotEncoder(sparse_output=False, drop='first')\n",
    "\n",
    "#Concating adultdata set and adulttest set\n",
    "cols_category = pd.concat([adultdata[cols_c], adulttest[cols_c]])\n",
    "\n",
    "e1.fit(cols_category)\n",
    "\n",
    "encoded_d = e1.transform(adultdata[cols_c])\n",
    "encoded_t = e1.transform(adulttest[cols_c])\n",
    "\n",
    "data_e = pd.DataFrame(encoded_d, columns=e1.get_feature_names_out(cols_c))\n",
    "test_e = pd.DataFrame(encoded_t, columns=e1.get_feature_names_out(cols_c))\n",
    "\n",
    "#Adding num cols\n",
    "num_cols = adultdata.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "data_e[num_cols] = adultdata[num_cols]\n",
    "test_e[num_cols] = adulttest[num_cols]\n",
    "\n",
    "data_e['income'] = adultdata['income'].map({'<=50K': 0, '>50K': 1})\n",
    "test_e['income'] = adulttest['income'].map({'<=50K': 0, '>50K': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate: 20.035624347398805\n",
      "Accuracy: 79.96437565260119\n"
     ]
    }
   ],
   "source": [
    "#Part 2 Implementation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data_a = data_e.drop(columns=['income'])\n",
    "data_b = data_e['income']\n",
    "test_a = test_e.drop(columns=['income'])\n",
    "test_b = test_e['income']\n",
    "\n",
    "# Initialize logistic regression model with 'liblinear' solver\n",
    "logistic_model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Training the model\n",
    "logistic_model.fit(data_a, data_b)\n",
    "\n",
    "# Predicting on the test data\n",
    "test_pred = logistic_model.predict(test_a)\n",
    "\n",
    "# Calculate classification error rate\n",
    "classification_error_rate = 1 - accuracy_score(test_b, test_pred)\n",
    "accuracy = accuracy_score(test_b ,test_pred)\n",
    "\n",
    "print(\"Error Rate:\", classification_error_rate*100)\n",
    "print(\"Accuracy:\", accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified Error Rate: 14.888520361157177\n",
      "Modified Accuracy  85.11147963884282\n"
     ]
    }
   ],
   "source": [
    "#Part 3 Implementation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature Engineering technique\n",
    "scaler = StandardScaler()\n",
    "data_a_scaled = scaler.fit_transform(data_a)\n",
    "test_a_scaled = scaler.transform(test_a)\n",
    "\n",
    "# Initializing logistic regression model\n",
    "logistic_model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'penalty': ['l1', 'l2']  # Penalty term ('l1': Lasso, 'l2': Ridge)\n",
    "}\n",
    "\n",
    "# Performing grid search with cross-validation\n",
    "grid_search = GridSearchCV(logistic_model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(data_a_scaled, data_b)\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_logistic_model = grid_search.best_estimator_\n",
    "\n",
    "# Predicting on the test data\n",
    "pred_logistic = best_logistic_model.predict(test_a_scaled)\n",
    "\n",
    "# Calculating classification error rate\n",
    "classification_error_rate_logistic = 1 - accuracy_score(test_b, pred_logistic)\n",
    "accuracy = accuracy_score(test_b ,pred_logistic)\n",
    "\n",
    "\n",
    "print(\"Modified Error Rate:\", classification_error_rate_logistic*100)\n",
    "print(\"Modified Accuracy \", accuracy*100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rates Summary for Different Percentages:\n",
      "X = 50% - MEAN: 0.200528, STD: 0.000218\n",
      "X = 60% - MEAN: 0.200319, STD: 0.000189\n",
      "X = 70% - MEAN: 0.200528, STD: 0.000361\n",
      "X = 80% - MEAN: 0.200418, STD: 0.000455\n",
      "X = 90% - MEAN: 0.200233, STD: 0.000198\n"
     ]
    }
   ],
   "source": [
    "#Part 4 implementation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "percentages = [50, 60, 70, 80, 90]\n",
    "\n",
    "# Initialize lists to store error rates for each X\n",
    "error_rates = {percentage: [] for percentage in percentages}\n",
    "\n",
    "num_iterations = 5\n",
    "\n",
    "for percentage in percentages:\n",
    "    for _ in range(num_iterations):\n",
    "        # Randomly sample the training dataset with the given percentage\n",
    "        data_a_sampled, _, y_train_sampled, _ = train_test_split(data_a, data_b, train_size=percentage/100, stratify=data_b)\n",
    "        \n",
    "        # Train the classifier (logistic regression) using the down-sampled training dataset\n",
    "        logistic_model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "        logistic_model.fit(data_a_sampled, y_train_sampled)\n",
    "        \n",
    "        # Evaluate the classifier using the evaluation dataset\n",
    "        b_pred = logistic_model.predict(test_a)\n",
    "        error_rate = 1 - accuracy_score(test_b, b_pred)\n",
    "        \n",
    "        # Record the error rate\n",
    "        error_rates[percentage].append(error_rate)\n",
    "\n",
    "# Calculate mean and deviation for each X\n",
    "mean_error_rates = {percentage: np.mean(error_rates[percentage]) for percentage in percentages}\n",
    "deviation_error_rates = {percentage: np.std(error_rates[percentage]) for percentage in percentages}\n",
    "\n",
    "# Print the results\n",
    "print(\"Error Rates Summary for Different Percentages:\")\n",
    "for percentage in percentages:\n",
    "    print(f\"X = {percentage}% - MEAN: {mean_error_rates[percentage]:.6f}, STD: {deviation_error_rates[percentage]:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 13.653952459922614\n",
      "Accuracy: 86.3460475400774\n"
     ]
    }
   ],
   "source": [
    "#Part 5 implementation\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [None, 5, 10, 15],  \n",
    "    'min_samples_split': [2, 3, 5, 8],  \n",
    "    'min_samples_leaf': [1, 2, 3, 4],  \n",
    "    'max_features': ['sqrt', 'log2', None]  \n",
    "}\n",
    "\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=7, scoring='f1')\n",
    "\n",
    "grid_search.fit(data_a, data_b)\n",
    "\n",
    "# Getting the best model\n",
    "best_decision_tree = grid_search.best_estimator_\n",
    "\n",
    "# Predictions for test set using the best model\n",
    "predictions_y = best_decision_tree.predict(test_a)\n",
    "\n",
    "accuracy_dt = accuracy_score(test_b, predictions_y)\n",
    "\n",
    "error_rate_dt = 1 - accuracy_dt\n",
    "print(f'Error rate: {error_rate_dt*100}')\n",
    "\n",
    "\n",
    "# Accuracy score for the best model\n",
    "print(f'Accuracy: {accuracy_dt*100}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
